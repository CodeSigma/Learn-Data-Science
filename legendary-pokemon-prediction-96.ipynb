{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Hi There!** I built some models to predict Legendary Pokemon using various algorithm, \n\nPlease give me some advice to improve my notebook since I'm still a beginner :)","metadata":{}},{"cell_type":"markdown","source":"# Read and Inspect the data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/pokemon/Pokemon.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There's no null value here, let's take the quantitative columns (numerical) and then plot their distributions","metadata":{}},{"cell_type":"code","source":"numerical = df.iloc[:,4:11]\nnumerical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical = df.iloc[:,4:11]\nfig, axes = plt.subplots(2,4, figsize=(20,10))\na = 0\nb = 0\nfor col in numerical.columns:\n    sns.distplot(numerical[col], ax = axes[a][b], kde=False )\n    b+=1\n    if b == 4:\n        b = 0\n        a += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(df[\"Legendary\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the distributions are right-skewed\n\nIt's probably due to the number of legendary pokemon is small compared to non-legendary pokemon, and legendary pokemon tends to have higher stats","metadata":{}},{"cell_type":"markdown","source":"Now, let's see the distribution of categorical features","metadata":{}},{"cell_type":"code","source":"categorical = df[[\"Type 1\", \"Type 2\", \"Generation\", \"Legendary\"]]\n\n\nfig, axes = plt.subplots(3, figsize=(10,20))\n# a = 0\nb = 0\nfor col in categorical.columns:\n    if col == \"Legendary\":\n        break\n    sns.countplot(y = categorical[col], hue= \"Legendary\", data = df, ax = axes[b])\n#     axes[a][b].set_xticklabels(axes[a][b].get_xticklabels(), rotation=90)\n    b+=1\n# ax = sns.countplot(df[\"Type 1\"])\n# ax.set_xticklabels(ax.get_xticklabels(), rotation=45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can't tell much, all of the categorical features seems to be independent","metadata":{}},{"cell_type":"markdown","source":"Now, let's inspect the plot some features to see how is the legendary pokemon's stats are dispersed","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.scatterplot(x=\"Total\", y=\"Attack\", hue=\"Legendary\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.swarmplot(x=\"Generation\", y=\"Total\", hue=\"Legendary\", data=df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As what I said before, legendary pokemons tend to have higher stats","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing and Prediction","metadata":{}},{"cell_type":"markdown","source":"A little preprocessing, so we can see the correlation between features","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder  #For convert categorical feature into number\nle = LabelEncoder()\ndf[\"Type 1\"] = le.fit_transform(df[\"Type 1\"])\ndf[\"Type 2\"] = df[\"Type 2\"].fillna(\"Null\") #Remove NaN values\ndf[\"Type 2\"] = le.fit_transform(df[\"Type 2\"])\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's see the correlations","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(16,9))\nsns.heatmap(df.corr(), annot = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All of the features have big correlations, so I'm going to use them all","metadata":{}},{"cell_type":"markdown","source":"Splitting dataset ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df.drop(['Name', 'Legendary'], axis = 1)\ny = df['Legendary']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0) #Split dataset into train and test set","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's predict without further preprocessing**","metadata":{}},{"cell_type":"code","source":"result = dict()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Just a simple logistic regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogR = LogisticRegression()\nlogR.fit(X_train, y_train)\ny_pred = logR.predict(X_test)\nres = logR.score(X_test, y_test)*100\nprint(res)\n\nresult[\"logR(NP)\"] = res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**88.75%**, not bad at all\n\nBut of course it can be better, let's do another preprocessing!\n","metadata":{}},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"I'm using transformer and pipeline, they're classes provided by scikit-learn for auto preprocessing and modelling so we don't have to write longer codes :D","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn import metrics\n\n\ncol_trans = make_column_transformer(\n            (OneHotEncoder(),['Type 1', 'Type 2', 'Generation']), #One Hot Encoder, re labelling type columns, because i heard that one hot encoder works better\n            (StandardScaler(),['Total','HP','Attack','Defense','Sp. Atk','Sp. Def','Speed']), #Scaling numeric data, so the training proccess will be better and faster\n            remainder = 'passthrough') #Not doing anything for the rest of columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's try another logistic regression, but with preprocessed data","metadata":{}},{"cell_type":"code","source":"pipe = make_pipeline(col_trans,logR)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n\nres = pipe.score(X_test, y_test)*100\nprint(res)\nresult[\"logR(P)\"] = res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, the accuracy increased 5% because of the preprocessing, **wow!**","metadata":{}},{"cell_type":"markdown","source":"# SVM\nNow, how about SVM?","metadata":{}},{"cell_type":"code","source":"#SVM\nfrom sklearn.svm import SVC\n\nsvm_model = SVC()\npipe = make_pipeline(col_trans, svm_model)\npipe.fit(X_train, y_train)\nres = pipe.score(X_test, y_test)*100\n\nprint(res)\nresult[\"SVM\"] = res\n# y_pred = pipe.predict(X_test)\n# print('Accuracy score on Test data: {}'.format(metrics.accuracy_score(y_test,y_pred)*100))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**90.625%**, not better than logistic regression","metadata":{}},{"cell_type":"markdown","source":"# Neural Network","metadata":{}},{"cell_type":"markdown","source":"F1-Score is a value for calculating Precision and Recall at the same time\n\nIn this case, it's just a bonus","metadata":{}},{"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n\ngamma = 2.0\nepsilon = K.epsilon()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's just use simple Neural Network, these layers work best so far","metadata":{}},{"cell_type":"code","source":"\nmodel = Sequential()\n\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(optimizer = Adam(1e-4),\n             loss = \"binary_crossentropy\",\n             metrics = [\"accuracy\", f1_m]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nearly_stop = EarlyStopping(patience=20, verbose=1, monitor='val_accuracy', mode='max')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(\n        X_train,\n        y_train,\n        epochs = 100,\n        callbacks=[early_stop],\n        validation_split=0.2,\n        verbose = 0\n    \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = model.evaluate(X_train, y_train)[1]*100\nresult[\"NN\"] = res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**92-94%,** not bad...","metadata":{}},{"cell_type":"markdown","source":"# Random Forest and Gradient Boosting","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)\npipe = make_pipeline(col_trans, rf)\npipe.fit(X_train, y_train)\nres = pipe.score(X_test, y_test)*100\n\nprint(res)\nresult[\"RF\"] = res\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(learning_rate=0.06, n_estimators = 100,validation_fraction = 0.2, n_iter_no_change=100)\npipe = make_pipeline(col_trans, gb)\npipe.fit(X_train, y_train)\nres = pipe.score(X_test, y_test)*100\n\nprint(res)\nresult[\"GB\"] = res","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These two work best so far, I will try another algorithm in the future","metadata":{}},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"list(result.values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(10,10))\nbar_plot = plt.bar(result.keys(), result.values())\n\ndef autolabel(rects):\n    x = 0\n    for idx,rect in enumerate(bar_plot):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()/2., 0.5*height,\n                round(list(result.values())[x], 2),\n                ha='center', va='bottom', rotation=0, size=20, color=\"white\")\n        x+=1\n\nautolabel(bar_plot)\nax.set_title(\"Prediction Result\", size=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\n**That's it!** Gradient Boosting work best for predicting legendary pokemon (so far)\n\nPlease kindly give me feedback and advice, so I can improve my notebook, thanks!! :)","metadata":{}}]}